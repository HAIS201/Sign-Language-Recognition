{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0038e5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로 설정\n",
    "DATASET_PATH = \"C:\\\\Users\\\\chaeyeonhan\\\\OneDrive\\\\2025-1\\\\capstone_design\"\n",
    "VIDEO_PATH = f\"{DATASET_PATH}/data/top100_videos_only\"\n",
    "TEST_VIDEO_PATH = f\"{DATASET_PATH}/data/test_videos\"\n",
    "LABEL_EXCEL = f\"{DATASET_PATH}/data/KETI_top100_clean_words.xlsx\"\n",
    "TEST_LABEL_EXCEL = f\"{DATASET_PATH}/data/test_data.xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad41fe3",
   "metadata": {},
   "source": [
    "# 전처리\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bef522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01. label 불러오기 및 영상번호 매핑\n",
    "import pandas as pd\n",
    "import os\n",
    "import unicodedata\n",
    "\n",
    "def load_label_info(label_excel_path):\n",
    "    label_df = pd.read_excel(label_excel_path)\n",
    "    label_df['파일ID'] = label_df['파일명'].apply(lambda x: os.path.splitext(x)[0])\n",
    "    video_to_label = dict(zip(label_df['파일ID'], label_df['한국어']))\n",
    "    labels = sorted(unicodedata.normalize(\"NFC\", l) for l in label_df['한국어'].unique())\n",
    "    label_map = {label: idx for idx, label in enumerate(labels)}\n",
    "    return label_df, label_map, video_to_label\n",
    "\n",
    "label_df, label_map, video_to_label = load_label_info(LABEL_EXCEL)\n",
    "test_label_df, test_label_map, test_video_to_label = load_label_info(TEST_LABEL_EXCEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626fe156",
   "metadata": {},
   "outputs": [],
   "source": [
    "#샘플링\n",
    "def sample_paths(npy_files, target_len=60):\n",
    "    total_len = len(npy_files)\n",
    "    if total_len >= target_len:\n",
    "        idxs = np.linspace(0, total_len - 1, target_len).astype(int)\n",
    "        return [npy_files[i] for i in idxs]\n",
    "    else:\n",
    "        pad_count = target_len - total_len\n",
    "        return npy_files + [npy_files[-1]] * pad_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634a58cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#샘플링할 때 datalist만드는 함수\n",
    "import numpy as np\n",
    "import os\n",
    "import unicodedata\n",
    "\n",
    "MP_DATA_PATH = \"C:\\\\Users\\\\chaeyeonhan\\\\OneDrive\\\\2025-1\\\\capstone_design\\\\data\\\\MP_Data\"\n",
    "MP_DATA_PATH_TEST = \"C:\\\\Users\\\\chaeyeonhan\\\\OneDrive\\\\2025-1\\\\capstone_design\\\\data\\\\MP_Data_Test\"\n",
    "\n",
    "def process_datalist(data_path, sequence_length=60):\n",
    "    data_list = []\n",
    "    for label in os.listdir(data_path):\n",
    "        label = unicodedata.normalize(\"NFC\", label)\n",
    "        label_path = os.path.join(data_path, label)\n",
    "        if not os.path.isdir(label_path):\n",
    "            continue\n",
    "        for video_id in os.listdir(label_path):\n",
    "            video_path = os.path.join(label_path, video_id)\n",
    "            npy_files = sorted(\n",
    "                [os.path.join(video_path, f) for f in os.listdir(video_path) if f.endswith(\".npy\")]\n",
    "            )\n",
    "\n",
    "            if len(npy_files) == 0:\n",
    "                continue\n",
    "\n",
    "            selected_files = sample_paths(npy_files, target_len=sequence_length)\n",
    "\n",
    "            label_idx = label_map.get(label)\n",
    "            if label_idx is None:\n",
    "                print(f\"Warning: Label '{label}' not found in label_map. Skipping this video.\")\n",
    "                continue\n",
    "\n",
    "            data_list.append((selected_files, label_idx))\n",
    "\n",
    "    return data_list\n",
    "\n",
    "train_data = process_datalist(MP_DATA_PATH)\n",
    "test_data = process_datalist(MP_DATA_PATH_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9860688",
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 증강강\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, Input, Masking, Attention, Bidirectional,GlobalAveragePooling1D, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "### 증강 함수들\n",
    "def add_gaussian_noise(data, std=0.01):\n",
    "    return data + np.random.normal(0, std, data.shape)\n",
    "\n",
    "def time_masking(data, max_mask_ratio=0.2):\n",
    "    masked = data.copy()\n",
    "    num_frames = data.shape[0]\n",
    "    num_mask = int(num_frames * max_mask_ratio)\n",
    "    mask_indices = np.random.choice(num_frames, num_mask, replace=False)\n",
    "    masked[mask_indices] = 0\n",
    "    return masked\n",
    "\n",
    "def normalize_frames(frames):\n",
    "    return [(f - np.mean(f)) / (np.std(f) + 1e-6) for f in frames]\n",
    "\n",
    "def smooth_labels(labels, smoothing=0.1):\n",
    "    n_classes = labels.shape[1]\n",
    "    return labels * (1 - smoothing) + (smoothing / n_classes)\n",
    "\n",
    "#속도 왜곡\n",
    "def time_warping(frames, stretch_factor=0.1):\n",
    "    length = len(frames)\n",
    "    factor = 1 + np.random.uniform(-stretch_factor, stretch_factor)\n",
    "    indices = np.linspace(0, length-1, int(length * factor)).astype(int)\n",
    "    indices = np.clip(indices, 0, length-1)\n",
    "    return np.array(frames)[indices]\n",
    "\n",
    "#각 keypoint 위치에 변동 추가\n",
    "def jitter(frames, sigma=0.01):\n",
    "    return frames + np.random.normal(loc=0, scale=sigma, size=frames.shape)\n",
    "\n",
    "\n",
    "#프레임 밀기\n",
    "def temporal_shift(seq, shift_max=5):\n",
    "    shift = np.random.randint(-shift_max, shift_max + 1)\n",
    "    if shift > 0:\n",
    "        return np.pad(seq[:-shift], ((shift, 0), (0, 0)), mode='edge')\n",
    "    elif shift < 0:\n",
    "        return np.pad(seq[-shift:], ((0, -shift), (0, 0)), mode='edge')\n",
    "    else:\n",
    "        return seq\n",
    "\n",
    "\n",
    "###  데이터 생성 + 증강 통합 함수\n",
    "def make_augmented_dataset(split_data, augment_multiplier=10, noise_std=0.01):\n",
    "    X, y = [], []\n",
    "    for npy_files, label in tqdm(split_data):\n",
    "        frames = [np.load(npy_file) for npy_file in npy_files]\n",
    "\n",
    "        frames = sample_paths(frames, target_len=60)\n",
    "\n",
    "        frames = normalize_frames(frames)\n",
    "        original = np.stack(frames)  # shape: (60, 1629)\n",
    "        X.append(original)\n",
    "        y.append(label)\n",
    "\n",
    "        aug_funcs = [add_gaussian_noise, time_masking, temporal_shift, jitter]\n",
    "\n",
    "        for _ in range(augment_multiplier):\n",
    "            aug = original.copy()\n",
    "            # 무작위로 1~2개 증강 선택 후 순서대로 적용\n",
    "            selected_augs = np.random.choice(aug_funcs, size=np.random.randint(1, 3), replace=False)\n",
    "            for func in selected_augs:\n",
    "                aug = func(aug)\n",
    "\n",
    "            # 프레임 수 보정 및 정규화\n",
    "            aug = sample_paths([f for f in aug], target_len=60)\n",
    "            aug = normalize_frames(aug)\n",
    "            aug = np.stack(aug)\n",
    "\n",
    "            X.append(aug)\n",
    "            y.append(label)\n",
    "\n",
    "      \n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42306ce",
   "metadata": {},
   "source": [
    "\n",
    "# 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7ba30a",
   "metadata": {},
   "source": [
    "## PCA의 최적 K값 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e79276",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def find_optimal_k_with_pca(X, threshold=0.95):\n",
    "    N, T, D = X.shape\n",
    "    X_flat = X.reshape(-1, D)\n",
    "\n",
    "    # PCA 전체 주성분 학습\n",
    "    pca = PCA(n_components=D)\n",
    "    pca.fit(X_flat)\n",
    "\n",
    "    # 누적 분산 설명 비율\n",
    "    cum_var_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "    # 임계값 이상이 되는 k 찾기\n",
    "    optimal_k = np.argmax(cum_var_ratio >= threshold) + 1\n",
    "\n",
    "    # 시각화\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(cum_var_ratio, marker='o')\n",
    "    plt.axhline(y=threshold, color='r', linestyle='--')\n",
    "    plt.axvline(x=optimal_k, color='g', linestyle='--')\n",
    "    plt.title(\"Cumulative Explained Variance Ratio\")\n",
    "    plt.xlabel(\"Number of Components (k)\")\n",
    "    plt.ylabel(\"Cumulative Variance\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\" {threshold*100:.0f}% 분산을 설명하기 위한 최적의 k: {optimal_k}\")\n",
    "    return optimal_k, pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a43e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(train_data, test_size=0.2, random_state=42,\n",
    "                              stratify=[label for _, label in train_data])\n",
    "\n",
    "X_train_raw, y_train = make_augmented_dataset(train, augment_multiplier=10)\n",
    "X_val_raw, y_val = make_augmented_dataset(val, augment_multiplier=0)\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n",
    "y_train = smooth_labels(y_train, smoothing=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed829045",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_k, pca_model = find_optimal_k_with_pca(X_train_raw, threshold=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a57d246",
   "metadata": {},
   "source": [
    "## 찾은 K로 PCA 적용해 학습 -> 하이퍼파라미터 튜너 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98338dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_tuner import HyperModel  # 수정됨\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, Input, LayerNormalization, Bidirectional, Attention, GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.decomposition import PCA\n",
    "import joblib\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1. 데이터 준비\n",
    "best_k = 450  # PCA로 줄인 feature 차원\n",
    "\n",
    "# 2. PCA 적용\n",
    "N_train, T, D = X_train_raw.shape\n",
    "X_train_flat = X_train_raw.reshape(-1, D)\n",
    "pca = PCA(n_components=best_k)\n",
    "X_train_pca_flat = pca.fit_transform(X_train_flat)\n",
    "X_train_k = X_train_pca_flat.reshape(N_train, T, best_k)\n",
    "\n",
    "N_val = X_val_raw.shape[0]\n",
    "X_val_flat = X_val_raw.reshape(-1, D)\n",
    "X_val_pca_flat = pca.transform(X_val_flat)\n",
    "X_val_k = X_val_pca_flat.reshape(N_val, T, best_k)\n",
    "\n",
    "# 3. 라벨 처리\n",
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n",
    "y_train = smooth_labels(y_train, smoothing=0.1)\n",
    "num_classes = y_train.shape[1]\n",
    "input_shape = (60, best_k)\n",
    "\n",
    "# 4. 튜닝 가능한 모델 정의\n",
    "class LSTMGRUHyperModel(HyperModel):\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def build(self, hp):\n",
    "        inputs = Input(shape=self.input_shape)\n",
    "\n",
    "        x = Bidirectional(LSTM(units=hp.Int('lstm_units', 32, 128, step=32), return_sequences=True))(inputs)\n",
    "        x = LayerNormalization()(x)\n",
    "        x = Dropout(hp.Float('lstm_dropout', 0.2, 0.5, step=0.1))(x)\n",
    "        \n",
    "        x = GRU(units=hp.Int('gru_units', 32, 128, step=32), return_sequences=True)(x)\n",
    "        x = LayerNormalization()(x)\n",
    "        x = Dropout(hp.Float('gru_dropout', 0.2, 0.5, step=0.1))(x)\n",
    "\n",
    "        x = Attention()([x, x])\n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "\n",
    "        x = Dense(units=hp.Int('dense_units', 32, 128, step=32), activation='relu')(x)\n",
    "        x = Dropout(hp.Float('gru_dropout', 0.2, 0.5, step=0.1))(x)\n",
    "        outputs = Dense(self.num_classes, activation='softmax')(x)\n",
    "\n",
    "        model = Model(inputs, outputs)\n",
    "        model.compile(\n",
    "            optimizer=Adam(hp.Float('lr', 1e-4, 1e-2, sampling='log')),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        return model\n",
    "\n",
    "# 5. 튜너 설정 및 탐색 시작\n",
    "hypermodel = LSTMGRUHyperModel(input_shape=input_shape, num_classes=num_classes)\n",
    "\n",
    "tuner = BayesianOptimization(\n",
    "    hypermodel,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=15,\n",
    "    executions_per_trial=1,\n",
    "    directory='keras_tuner_dir',\n",
    "    project_name=f'sign_tune_pca{best_k}',\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "lr_schedule = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-5,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "tuner.search(X_train_k, y_train,\n",
    "             validation_data=(X_val_k, y_val),\n",
    "             epochs=150,\n",
    "             batch_size=64,\n",
    "             callbacks=[EarlyStopping(patience=10, restore_best_weights=True), lr_schedule],\n",
    "             verbose=1)\n",
    "\n",
    "# 6. 튜닝 결과 저장\n",
    "def extract_trials_to_dataframe(tuner, sort_by=\"score\", top_n=None):\n",
    "    trial_data = []\n",
    "    for trial in tuner.oracle.trials.values():\n",
    "        row = trial.hyperparameters.values.copy()\n",
    "        row[\"score\"] = trial.score\n",
    "        trial_data.append(row)\n",
    "    df = pd.DataFrame(trial_data)\n",
    "    df = df.sort_values(by=sort_by, ascending=False).reset_index(drop=True)\n",
    "    if top_n:\n",
    "        df = df.head(top_n)\n",
    "    return df\n",
    "\n",
    "df = extract_trials_to_dataframe(tuner, top_n=15)\n",
    "df.to_csv(\"tuner_results_pca.csv\", index=False)\n",
    "\n",
    "# 7. 최적 모델/하이퍼파라미터 저장\n",
    "best_model = tuner.get_best_models(1)[0]\n",
    "best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "joblib.dump(pca, f\"{DATASET_PATH}/pca_{best_k}.joblib\")\n",
    "\n",
    "val_acc = best_model.evaluate(X_val_k, y_val, verbose=0)[1]\n",
    "print(f\" [PCA {best_k}] 튜너 기반 최적 모델의 검증 정확도: {val_acc:.4f}\")\n",
    "print(\" Best Hyperparameters:\", best_hp.values)\n",
    "\n",
    "if not os.path.exists(f\"{DATASET_PATH}/models\"):\n",
    "    os.makedirs(f\"{DATASET_PATH}/models\")\n",
    "best_model.save(f\"{DATASET_PATH}/models/best_pca{best_k}_hypertuning.keras\") from keras_tuner.tuners import BayesianOptimization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d98d10",
   "metadata": {},
   "source": [
    "## 찾은 best hyperparameter로 다시 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4eccf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LayerNormalization\n",
    "\n",
    "# 1. 증강 데이터 생성\n",
    "train, val = train_test_split(train_data, test_size=0.2, random_state=42,\n",
    "                              stratify=[label for _, label in train_data])\n",
    "X_train_aug_raw, y_train_aug = make_augmented_dataset(train, augment_multiplier=10)\n",
    "X_val_raw, y_val = make_augmented_dataset(val, augment_multiplier=0)\n",
    "\n",
    "# 2. PCA 450차원 적용\n",
    "N_train, T, D = X_train_aug_raw.shape\n",
    "X_train_flat = X_train_aug_raw.reshape(-1, D)\n",
    "\n",
    "pca = PCA(n_components=450)\n",
    "X_train_pca_flat = pca.fit_transform(X_train_flat)\n",
    "X_train_aug = X_train_pca_flat.reshape(N_train, T, 450)\n",
    "\n",
    "# validation 데이터도 같은 PCA로 변환\n",
    "N_val = X_val_raw.shape[0]\n",
    "X_val_flat = X_val_raw.reshape(-1, D)\n",
    "X_val_pca_flat = pca.transform(X_val_flat)\n",
    "X_val = X_val_pca_flat.reshape(N_val, T, 450)\n",
    "\n",
    "# 3. 라벨 처리\n",
    "num_classes = y_train.shape[1]\n",
    "y_train_aug = to_categorical(y_train_aug, num_classes=num_classes)\n",
    "y_train_aug = smooth_labels(y_train_aug, smoothing=0.1)\n",
    "y_val = to_categorical(y_val, num_classes=num_classes)\n",
    "\n",
    "# 4. best_hp 기반 모델 정의 (input shape 수정)\n",
    "input_shape = (60, 450)\n",
    "model_input = Input(shape=input_shape)\n",
    "\n",
    "x = Bidirectional(LSTM(units=best_hp.get('lstm_units'), return_sequences=True))(model_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(best_hp.get('lstm_dropout'))(x)\n",
    "\n",
    "x = GRU(units=best_hp.get('gru_units'), return_sequences=True)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(best_hp.get('gru_dropout'))(x)\n",
    "\n",
    "x = Attention()([x, x])\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "\n",
    "x = Dense(units=best_hp.get('dense_units'), activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(model_input, output)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=best_hp.get('lr')),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 5. 학습\n",
    "early_stop = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "lr_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-5, verbose=1)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_aug, y_train_aug,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=150,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop, lr_schedule],\n",
    "    verbose=1\n",
    ")\n",
    "joblib.dump(pca, f\"{DATASET_PATH}/pca_{best_k}_retrain.joblib\")\n",
    "model.save('best_model_pca450.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4062c0",
   "metadata": {},
   "source": [
    "# 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24bb222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 1. PCA 및 모델 로드\n",
    "pca = joblib.load(f\"{DATASET_PATH}/pca_450.joblib\")\n",
    "model = load_model(f\"{DATASET_PATH}/models/best_pca450_hypertuning.keras\")\n",
    "\n",
    "# 2. test 데이터 준비 및 PCA 적용\n",
    "X_test_raw, y_test = make_augmented_dataset(test_data, augment_multiplier=0)\n",
    "N_test, T, D = X_test_raw.shape\n",
    "X_test_flat = X_test_raw.reshape(-1, D)\n",
    "X_test_pca_flat = pca.transform(X_test_flat)\n",
    "X_test = X_test_pca_flat.reshape(N_test, T, 450)\n",
    "\n",
    "# 3. 라벨 처리\n",
    "y_test = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "# 4. 평가\n",
    "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"✅ PCA 기반 테스트 정확도: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e036bf",
   "metadata": {},
   "source": [
    "## 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b994b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA 테스트\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='Train')\n",
    "plt.plot(history.history['val_accuracy'], label='Val')\n",
    "plt.legend(); plt.title('Accuracy')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
